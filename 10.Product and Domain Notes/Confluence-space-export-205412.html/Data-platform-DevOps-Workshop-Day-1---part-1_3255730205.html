<!DOCTYPE html>
<html>
    <head>
        <title>Jackson Fan : Data platform DevOps Workshop Day 1 - part 1</title>
        <link rel="stylesheet" href="styles/site.css" type="text/css" />
        <META http-equiv="Content-Type" content="text/html; charset=UTF-8">
    </head>

    <body class="theme-default aui-theme-default">
        <div id="page">
            <div id="main" class="aui-page-panel">
                <div id="main-header">
                    <div id="breadcrumb-section">
                        <ol id="breadcrumbs">
                            <li class="first">
                                <span><a href="index.html">Jackson Fan</a></span>
                            </li>
                                                    <li>
                                <span><a href="JF-Space-Overview_2554888272.html">JF Space Overview</a></span>
                            </li>
                                                </ol>
                    </div>
                    <h1 id="title-heading" class="pagetitle">
                                                <span id="title-text">
                            Jackson Fan : Data platform DevOps Workshop Day 1 - part 1
                        </span>
                    </h1>
                </div>

                <div id="content" class="view">
                    <div class="page-metadata">
                            
        
    
        
    
        
        
            Created by <span class='author'> Jackson Fan</span>, last modified on 17-07-25
                        </div>
                    <div id="main-content" class="wiki-content group">
                    <h2 id="DataplatformDevOpsWorkshopDay1-part1-DataPlatformPipelineFramework–End-to-EndArchitectureandDeployment">Data Platform Pipeline Framework – End-to-End Architecture and Deployment</h2><h2 id="DataplatformDevOpsWorkshopDay1-part1-HighLevel">High Level </h2><h2 id="DataplatformDevOpsWorkshopDay1-part1-OverviewandEnvironmentSetup">Overview and Environment Setup</h2><p>This document provides a comprehensive overview of the <strong>Data Platform pipeline framework</strong> as discussed in the DEV Workshop on 17/07/2025. </p><p>It covers the end-to-end process from setting up Linked Services in Azure Synapse Analytics through to the detailed inner workings of pipelines, including how DevOps deployment is handled across multiple environments. The goal is to understand how data is ingested from various sources into an <strong>Operational Data Store (ODS)</strong> using Azure Synapse Pipelines, managed via a robust metadata-driven framework and automated deployment process.</p><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-040922.png" width="760" loading="lazy" src="attachments/3255730205/3256156170.png?width=760" data-image-src="attachments/3255730205/3256156170.png" data-height="757" data-width="1016" data-unresolved-comment-count="0" data-linked-resource-id="3256156170" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-040922.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="d56703db-d9b9-4254-9102-bde372cc1c15" data-media-type="file"></span><p /><p><strong>Environment Structure:</strong> The Data Platform spans <strong>four environments</strong> – <strong>DEV</strong>, <strong>UAT</strong>, <strong>PREPROD</strong>, and <strong>PROD</strong>. </p><p>Development and changes are done in DEV, and then propagated to higher environments through a CI/CD process. </p><p>Best practice is that all changes (pipeline updates, database schema changes, etc.) are initially made in DEV. In rare cases of emergency hotfixes applied directly in PROD, those changes should be back-ported to lower environments (PREPROD, UAT, DEV) to keep all environments in sync. This avoids divergence where a fix in PROD might be overwritten by the next release from DEV if not synchronized.</p><p><strong>Source Control Integration:</strong> </p><p>In the DEV Synapse workspace, <strong>Git integration</strong> is enabled. All pipelines, datasets, and other Synapse objects are stored as JSON code in an Azure DevOps Git repository. </p><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-040932.png" width="314" loading="lazy" src="attachments/3255730205/3255861286.png?width=314" data-image-src="attachments/3255730205/3255861286.png" data-height="513" data-width="314" data-unresolved-comment-count="0" data-linked-resource-id="3255861286" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-040932.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="36efab01-8a2e-463f-8711-6234a8c5ce2c" data-media-type="file"></span><p /><p>Typically, the workspace is connected to a project repository (for example, “Bedrock-BI-Synapse” as shown in the workshop) with a main collaboration branch (often named <strong>main</strong>) </p><p>Developers create feature branches (e.g. named with initials or work item IDs (Branch ID) like <code>AP_20231206_UpdateRestAPI</code>, <code>BIGG-568</code>, etc.) </p><p>for any significant changes. These branches follow a consistent naming convention to easily identify their purpose or the work item. Before deployment, changes are merged into the main branch and <strong>published</strong> to generate an ARM template for release. </p><p>Always double-check that you are working on the correct Git branch in Synapse Studio (shown at the top of the Synapse interface) before making edits, to ensure changes go to the intended branch.</p><p><strong>Deployment to Higher Environments:</strong> </p><p>The promotion from DEV to other environments is handled by Azure DevOps pipelines using the generated ARM templates. Each environment (DEV, UAT, PREPROD, PROD) has a corresponding ARM template parameters file. During the deployment (CI/CD) pipeline, environment-specific values (like resource names, service endpoints, etc.) are supplied via <strong>override parameters</strong> in the deployment YAML (This could find in Part 2). </p><p>This mechanism ensures that, for example, a linked service pointing to a DEV database gets updated to point to the UAT database when deploying to UAT, without manually editing JSON. </p><p>The use of parameter override keeps the deployment process flexible – however, it does require maintaining the parameter files or override scripts as the project grows. As a best practice, if there are many environment-specific parameters, consider using <strong>Azure Data Factory/Synapse global parameters</strong> or external configuration files to manage them, so that the DevOps pipeline does not need constant manual updates for new parameters.</p><blockquote><p><strong>Note:</strong> Triggers (scheduled pipeline triggers) are typically <em>not</em> enabled in lower environments or source-controlled in the same way as pipelines. Often, triggers are set up only in higher environments or are parameterized. In this project, triggers have consistent names and configurations across environments, but one should be cautious about automatically deploying triggers – you may want to disable them in DEV/UAT to avoid accidental runs. Naming triggers clearly (e.g., including environment or function in the name) is recommended to avoid confusion when overriding schedules in DevOps.</p></blockquote><h2 id="DataplatformDevOpsWorkshopDay1-part1-Step1-LinkedServicesandIntegrationRuntimes">Step 1 - Linked Services and Integration Runtimes</h2><p>Linked Services in Synapse pipelines act like connection strings, defining how the service connects to external data sources (databases, APIs, file storage, etc.). </p><p>Setting up linked services is typically the <strong>first step</strong> in configuring the pipelines for a new data source. </p><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-040949.png" width="760" loading="lazy" src="attachments/3255730205/3256090637.png?width=760" data-image-src="attachments/3255730205/3256090637.png" data-height="703" data-width="1394" data-unresolved-comment-count="0" data-linked-resource-id="3256090637" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-040949.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="1158a7a2-a0ae-49d3-bfa5-6388dbb9d318" data-media-type="file"></span><p /><p>In Synapse Studio, linked services are managed in the <strong>Manage</strong> hub under “External connections.” Each linked service specifies a data source type, connection info, and authentication method. For example, there are linked services for sources like <strong>CRM</strong>, <strong>DataServices</strong>, <strong>Fusion</strong>, <strong>MMC</strong>, <strong>NZXWT</strong> (each likely a code name for a source system), </p><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041139.png" width="340" loading="lazy" src="attachments/3255730205/3255730219.png?width=340" data-image-src="attachments/3255730205/3255730219.png" data-height="711" data-width="340" data-unresolved-comment-count="0" data-linked-resource-id="3255730219" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041139.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="27520391-1d71-4028-acc1-1f4b421361e9" data-media-type="file"></span><p /><p>as well as target systems (e.g., a <strong>Target Synapse SQL pool</strong>, or a <strong>Target MSDB</strong>) as seen in the workspace <em>Example of multiple linked services configured in the Synapse workspace, each corresponding to an external source or target system.</em> </p><p>These names usually follow a naming convention such as <code>ls_Source_&lt;SystemName&gt;</code> or <code>ls_Target&lt;...&gt;</code> to clearly indicate their purpose.</p><p><strong>Integration Runtime (IR):</strong> For each linked service, you must specify an <strong>Integration Runtime</strong>. This can be the default Azure integration runtime (for cloud services) or a <strong>Self-Hosted IR</strong> for on-premises sources. In our framework, many data sources are on-premises SQL Server databases, so a self-hosted IR (e.g. <strong>OnPrem-IR1</strong>) is used. </p><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041000.png" width="760" loading="lazy" src="attachments/3255730205/3255828519.png?width=760" data-image-src="attachments/3255730205/3255828519.png" data-height="714" data-width="1383" data-unresolved-comment-count="0" data-linked-resource-id="3255828519" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041000.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="ff19ceee-75f0-4950-b3f1-0e1d5c65ffd3" data-media-type="file"></span><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041016.png" width="657" loading="lazy" src="attachments/3255730205/3255959565.png?width=657" data-image-src="attachments/3255730205/3255959565.png" data-height="117" data-width="657" data-unresolved-comment-count="0" data-linked-resource-id="3255959565" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041016.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="6ec08fe2-f7f6-4ba0-89e4-a7c493e7a15b" data-media-type="file"></span><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041020.png" width="703" loading="lazy" src="attachments/3255730205/3255795733.png?width=703" data-image-src="attachments/3255730205/3255795733.png" data-height="644" data-width="703" data-unresolved-comment-count="0" data-linked-resource-id="3255795733" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041020.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="f7651e88-ed0d-47b2-97a1-f6b4a2513f6e" data-media-type="file"></span><p /><p>The self-hosted IR runs on a VM or server within the network, enabling Synapse to securely pull data from on-prem systems. Note that the IR can become a <strong>performance bottleneck</strong> if it handles many large data transfers concurrently – it’s essentially a gateway. Ensure the IR has sufficient resources or scale-out nodes if large volumes of data are moved, to avoid it throttling the pipeline throughput. In the screenshot below, an example linked service is shown using a self-hosted IR (OnPrem-IR1) which at the moment is offline/unavailable (hence the error). The IR must be running and connected for pipelines to use it successfully.</p><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041156.png" width="760" loading="lazy" src="attachments/3255730205/3256123423.png?width=760" data-image-src="attachments/3255730205/3256123423.png" data-height="107" data-width="1216" data-unresolved-comment-count="0" data-linked-resource-id="3256123423" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041156.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="09e9e2b1-a379-4097-a032-8161272b1a55" data-media-type="file"></span><p /><p><strong>Secure Credential Management with Key Vault:</strong> </p><p>We avoid embedding secrets (passwords, keys) directly in linked service definitions. Instead, each linked service that requires credentials uses an <strong>Azure Key Vault</strong> reference. In the linked service configuration, Synapse allows selecting <strong>&quot;Azure Key Vault&quot;</strong> as the authentication option for the connection string or password. In our project, a Key Vault linked service (named something like <code>bikeyvault</code>) is created, which points to the actual Azure Key Vault service (one per environment). The linked service then references a specific <strong>secret name</strong> in that vault to get the actual connection string or credential at runtime. For example, a SQL Server linked service for source AACPR might use a secret named &quot;SourceAACPR&quot; in the Key Vault to retrieve the DB connection string or password <em>Configuration of a SQL Server linked service using a self-hosted IR and Azure Key Vault for credentials. The Key Vault (e.g., “bikeyvault”) stores the connection string as a secret (</em><code>SourceAACPR</code><em>), which the linked service fetches at runtime.</em></p><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041034.png" width="632" loading="lazy" src="attachments/3255730205/3255795740.png?width=632" data-image-src="attachments/3255730205/3255795740.png" data-height="306" data-width="632" data-unresolved-comment-count="0" data-linked-resource-id="3255795740" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041034.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="c69ffba6-25b2-4478-a545-198eaacea623" data-media-type="file"></span><p /><p>Each environment has its own Key Vault (for instance, a Key Vault for Dev, one for UAT, etc., each with appropriate naming or tags indicating the environment). This way, when deploying the linked services to a different environment, the only thing that changes is the <strong>Key Vault base URL</strong> (and possibly the secret names if they differ). In practice, the DevOps deployment pipeline will override the Key Vault linked service’s URL to point to the environment’s vault. The secrets inside each vault are named consistently (e.g., each vault contains <code>SourceCRM</code>, <code>SourceDataServices</code>, etc., with values appropriate for that environment). This setup greatly simplifies credential management: to switch environments, you don’t need to edit dozens of connection strings – you just point to a different Key Vault, and the vault provides the correct secrets for that stage.</p><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041042.png" width="760" loading="lazy" src="attachments/3255730205/3255861292.png?width=760" data-image-src="attachments/3255730205/3255861292.png" data-height="763" data-width="1656" data-unresolved-comment-count="0" data-linked-resource-id="3255861292" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041042.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="f462851e-dfbe-4462-85ad-d00db1a370ef" data-media-type="file"></span><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041050.png" width="671" loading="lazy" src="attachments/3255730205/3255959571.png?width=671" data-image-src="attachments/3255730205/3255959571.png" data-height="284" data-width="671" data-unresolved-comment-count="0" data-linked-resource-id="3255959571" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041050.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="2d36c870-c0a5-404f-a696-664ddd92e3aa" data-media-type="file"></span><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041058.png" width="649" loading="lazy" src="attachments/3255730205/3255828525.png?width=649" data-image-src="attachments/3255730205/3255828525.png" data-height="724" data-width="649" data-unresolved-comment-count="0" data-linked-resource-id="3255828525" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041058.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="2ff9986f-77f7-45d3-b650-213b0a45c061" data-media-type="file"></span><p /><p /><blockquote><p><strong>Note:</strong> Not all linked service types seamlessly integrate with Key Vault or parameterization. For standard SQL and storage connections, Key Vault works well. However, for certain sources like REST APIs or others, you might end up manually overriding parameters (like Base URLs, API keys) in the ARM template if Key Vault integration isn’t straightforward. The DevOps YAML can handle these overrides for each environment. It’s important to document any such special cases so that when promoting to Test/Prod, those values are correctly substituted.</p></blockquote><h2 id="DataplatformDevOpsWorkshopDay1-part1-Step2-PipelineTriggersandScheduling">Step 2 - Pipeline Triggers and Scheduling</h2><p>Triggers in Synapse (similar to Azure Data Factory) are used to schedule pipeline executions or respond to events. In this data platform, <strong>triggers are primarily time-based (scheduled)</strong> to run the <strong>master pipelines</strong> at defined intervals. For example, there may be nightly loads, intraday loads, or monthly schedules for certain processes.</p><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041209.png" width="684" loading="lazy" src="attachments/3255730205/3255828531.png?width=684" data-image-src="attachments/3255730205/3255828531.png" data-height="728" data-width="684" data-unresolved-comment-count="0" data-linked-resource-id="3255828531" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041209.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="e452ab26-7653-4a41-b4c4-f13344dce204" data-media-type="file"></span><p /><p>Common trigger types used:</p><ul><li><p><strong>Schedule Trigger:</strong> Runs pipelines at a specific clock time or frequency (e.g., every day at 2 AM). This is straightforward for daily or hourly jobs.</p></li><li><p><strong>Tumbling Window Trigger:</strong> Useful for scenarios where the trigger should fire at specific periodic intervals <em>with a dependency on previous run completion</em>. Tumbling window triggers can also be configured to catch up on missed intervals. They are handy for more complex schedules like <em>“every X business days”</em> or intraday jobs that must wait for prior jobs. For instance, a use-case discussed was running a process on the <strong>6th NZ working day of every month</strong> (to align with business day schedules). This is not a simple cron expression, so a tumbling window or a custom solution is used – possibly the trigger fires on a regular interval and the pipeline contains logic to check if “today is the 6th working day” and only proceed if true. Tumbling triggers ensure that if a scheduled run is missed (perhaps due to downtime), it can be executed later so no interval is skipped, which is important for strict monthly cycles.</p></li><li><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041306.png" width="736" loading="lazy" src="attachments/3255730205/3255926803.png?width=736" data-image-src="attachments/3255730205/3255926803.png" data-height="548" data-width="856" data-unresolved-comment-count="0" data-linked-resource-id="3255926803" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041306.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="c14fbadb-af28-4b0d-aae4-47b122e22335" data-media-type="file"></span><p /></li><li><p><strong>Event-based Trigger:</strong> Not heavily emphasized in this framework, but in some cases one might have triggers listening to events (like file arrival in blob storage). Most of the data sources here are databases, so time triggers are the primary method.</p></li></ul><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041223.png" width="636" loading="lazy" src="attachments/3255730205/3255861299.png?width=636" data-image-src="attachments/3255730205/3255861299.png" data-height="263" data-width="636" data-unresolved-comment-count="0" data-linked-resource-id="3255861299" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041223.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="790ca913-a612-4f0e-bebe-e884160a2172" data-media-type="file"></span><p /><p>The <strong>Master pipeline</strong> (or pipelines) that orchestrate the loads are associated with these triggers. It’s recommended <strong>not to overly proliferate triggers</strong>. In some projects, every pipeline has its own trigger (leading to 20+ triggers), but here a more centralized approach is preferred: e.g., a single nightly trigger might kick off a master process that handles all sources, or a couple of triggers handle different sub-systems (one for intraday, one for end-of-day, etc.). This approach makes it easier to manage schedule changes and monitor runs.</p><p><strong>Trigger Deployment:</strong> Typically, triggers are included in the ARM deployment (unless deliberately excluded). When deploying to a new environment, one must ensure the trigger’s schedule is appropriate for that environment (for example, you might not want DEV to run automatically at midnight). Often triggers can be deployed in a stopped state and then enabled in Prod. The key is to <strong>name triggers consistently</strong> and possibly have environment-specific adjustments via parameters if needed. In our setup, the trigger definitions exist, but operations like enabling/disabling or adjusting times can be done post-deployment or via parameter overrides.</p><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041320.png" width="760" loading="lazy" src="attachments/3255730205/3255795746.png?width=760" data-image-src="attachments/3255730205/3255795746.png" data-height="774" data-width="1618" data-unresolved-comment-count="0" data-linked-resource-id="3255795746" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041320.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="21a6af41-5e9b-4825-80c8-684becfe03ed" data-media-type="file"></span><p /><h2 id="DataplatformDevOpsWorkshopDay1-part1-Step3-DatasetsandDataFormats">Step 3 - Datasets and Data Formats</h2><p>Within Synapse pipelines, <strong>datasets</strong> represent data structures (tables, files, etc.) and are used by activities (like Copy) to read from or write to data stores via the linked services. The project uses a <strong>parameterized dataset pattern</strong> to avoid creating hundreds of dataset objects. Instead of making a separate dataset for every table, we create a generic dataset per source (or per type of source) and use parameters for things like table name or file path.</p><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041331.png" width="369" loading="lazy" src="attachments/3255730205/3255926810.png?width=369" data-image-src="attachments/3255730205/3255926810.png" data-height="686" data-width="369" data-unresolved-comment-count="0" data-linked-resource-id="3255926810" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041331.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="8287ffb5-e7a2-4141-8713-17edb68b9618" data-media-type="file"></span><p /><p>For example, for a relational source like the CRM database, there might be a dataset <code>ds_CRM</code> that has parameters for <code>SchemaName</code> and <code>TableName</code>. This single dataset can represent any table in the CRM source by passing different values for those parameters at runtime. The dataset’s connection is tied to <code>ls_Source_CRM</code> (the CRM linked service), and in the dataset definition the <strong>Table property</strong> is set to <code>@dataset().SchemaName.@dataset().TableName</code> (concatenating schema and table) <em>Example dataset configuration using parameters for schema and table name. The linked service </em><code>ls_Source_ACE_MEB</code><em> is used, and the Table is specified dynamically via </em><code>@dataset().SchemaName</code><em> and </em><code>@dataset().TableName</code><em>.</em>. The actual values for these dataset parameters will be supplied by the pipeline at runtime (coming from the metadata framework, as we will see later). This dynamic approach is crucial for scalability: if there are <strong>2,000+ tables</strong> across sources to ingest, defining each one manually would be unmaintainable.</p><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041342.png" width="760" loading="lazy" src="attachments/3255730205/3256090644.png?width=760" data-image-src="attachments/3255730205/3256090644.png" data-height="233" data-width="1039" data-unresolved-comment-count="0" data-linked-resource-id="3256090644" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041342.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="772b6477-7279-44be-baac-ab680e9718bf" data-media-type="file"></span><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041501.png" width="760" loading="lazy" src="attachments/3255730205/3255992349.png?width=760" data-image-src="attachments/3255730205/3255992349.png" data-height="691" data-width="1601" data-unresolved-comment-count="0" data-linked-resource-id="3255992349" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041501.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="0b6ab357-dacd-4d7b-8902-c59371008d01" data-media-type="file"></span><p /><p><strong>Handling Different File Types:</strong> The datasets also handle various file formats for sources that are file-based (though much of the current data sources seem to be databases). If a source were a CSV or Parquet file in Data Lake Storage, the dataset would define properties like delimiter, compression type (e.g., CSV with GZip compression, or Parquet with Snappy compression). The framework enforces conventions for these to avoid run-time surprises. For instance, CSV datasets should specify the exact column delimiter and text qualifier, and whether the first row is header or not, to ensure consistency. Parquet datasets typically don’t need such settings beyond compression codec. In our notes, it was mentioned that <strong>CSV sources need strict guidance</strong> due to various edge cases (commas within data, etc.), so if any CSV ingestion is added, one should follow the existing pattern/template for CSV datasets.</p><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041510.png" width="760" loading="lazy" src="attachments/3255730205/3255828538.png?width=760" data-image-src="attachments/3255730205/3255828538.png" data-height="688" data-width="1694" data-unresolved-comment-count="0" data-linked-resource-id="3255828538" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041510.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="c25f4b9d-eba7-48a8-bd58-5f7e982c30d1" data-media-type="file"></span><p /><p><strong>No Default Values in Datasets:</strong> During setup, it’s noted to <em>leave dataset default values blank</em> for parameters like table name. This is intentional – we don’t want the dataset tied to a specific table by default. Instead, always pass the parameter when using the dataset. This prevents any accidental usage of a wrong default and also serves as a check: if a dataset is executed without a parameter, it will error, alerting us to a misconfigured pipeline. It essentially forces the pipelines to supply a table name and schema.</p><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041523.png" width="760" loading="lazy" src="attachments/3255730205/3256025130.png?width=760" data-image-src="attachments/3255730205/3256025130.png" data-height="691" data-width="1093" data-unresolved-comment-count="0" data-linked-resource-id="3256025130" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041523.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="cf803155-95a1-4f8d-ae46-54781e97d5a8" data-media-type="file"></span><p /><h2 id="DataplatformDevOpsWorkshopDay1-part1-Step4-PipelineFrameworkArchitecture">Step 4 - Pipeline Framework Architecture</h2><p>The core of the framework is a set of pipelines that orchestrate the end-to-end load of data from sources into the target system. </p><p>The design is <strong>metadata-driven</strong>, meaning that most of the pipeline logic is generic and driven by configuration data (stored in a <strong>Framework database</strong>). </p><p>In the workshop notes, references are made to tables like <code>BIML.DataLoadDetails</code>, <code>BIML.DataLoad</code>, <code>[Metadata].[DataSources]</code>, etc., which suggests there is a SQL database (often called a <strong>Framework DB</strong> or <strong>Metadata DB</strong>) that contains all the information about what to load, from where, and how.</p><h3 id="DataplatformDevOpsWorkshopDay1-part1-MasterPipeline(OrchestrationLayer)">Master Pipeline (Orchestration Layer)</h3><p>At the top level, there is a <strong>master orchestration pipeline</strong> (perhaps named <strong>Master - Execute Process</strong> or similar). This pipeline's job is to coordinate the overall process. It typically performs tasks such as:</p><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041534.png" width="760" loading="lazy" src="attachments/3255730205/3255992355.png?width=760" data-image-src="attachments/3255730205/3255992355.png" data-height="763" data-width="1700" data-unresolved-comment-count="0" data-linked-resource-id="3255992355" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041534.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="0d34ab21-3b07-41e4-b488-c9e8b30425c1" data-media-type="file"></span><p /><ol start="1"><li><p><strong>Initiating a Logging Batch:</strong> When the pipeline run starts, it creates a new <strong>batch record</strong> in the log table (e.g., an ETL Batch table in the framework DB) to mark the start of a load batch. This might be done via a stored procedure activity that calls something like <code>[ETL].[StartBatch]</code> which returns a new Batch ID. This Batch ID will be used to tie together all log entries for this run.</p></li><li><p><strong>Executing Sub-Pipelines in Sequence or Parallel:</strong> The master pipeline will use <strong>Execute Pipeline</strong> activities to call other pipelines. In the screenshot below, we can see an example master pipeline canvas where multiple Execute Pipeline activities are orchestrated <em>Portion of the master pipeline (&quot;Master - Execute Process&quot;) orchestrating sub-processes: e.g., executing “Load Sources – ALL”, then “Load Staging”, then dimension/fact loads, and finally sending an email and running a circuit-breaker check.</em>. The red arrows highlight some of the Execute Pipeline steps:</p><ul><li><p><strong>Load Sources – ALL:</strong> This is likely a pipeline that handles extracting data from all source systems into the ODS (staging area). It might loop through each configured source (CRM, Bloomberg, etc.) and trigger their loads. This could also run in parallel for multiple sources if designed that way.</p></li><li><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041545.png" width="712" loading="lazy" src="attachments/3255730205/3255926817.png?width=712" data-image-src="attachments/3255730205/3255926817.png" data-height="457" data-width="1597" data-unresolved-comment-count="0" data-linked-resource-id="3255926817" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041545.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="e4d95861-1b3a-4266-8cb2-137b5fd9a9dc" data-media-type="file"></span><p /></li><li><p><strong>Load Staging:</strong> This could be a pipeline that takes data from the ODS (landing tables) and transforms/loads it into an integrated target (like a data warehouse or a consolidated schema). However, given the context, it might also refer to loading a staging area for further processing. (In some projects, “Load Sources” brings raw data into staging tables, and then a separate step merges those into final tables).</p></li><li><p><strong>Load Dimensions and Facts:</strong> The screenshot text was cut, but it appears there is an activity for loading dimensions (and facts). This suggests a subsequent step where, after raw data is in the ODS, the data warehouse dimension and fact tables are being processed (perhaps using stored procedures or mappings).</p></li><li><p><strong>Send Email (Alert_SendMessage):</strong> A notification pipeline to send out an email (likely summarizing the load status or alerting on failure). It's common to have an email at end of master pipeline to notify success or issues.</p></li><li><p><strong>Run Circuit Breaker:</strong> A pipeline to handle any <em>circuit breaker</em> logic. A circuit breaker in data pipelines could mean if too many failures occur (or some threshold is exceeded, like data quality issues), then subsequent triggers are halted until manually reset. This pipeline might, for example, disable certain triggers or update a config that prevents the next run if the current run encountered critical issues. It’s a safety mechanism to avoid bad data propagating.</p></li></ul></li><li><p><strong>Coordinating Dependencies:</strong> The master pipeline likely has dependencies between these steps (as shown by the green arrows in the canvas). For example, it might run <strong>Load Sources – ALL</strong> first. Only when all source loads complete (successfully) does it proceed to <strong>Load Staging</strong>. After staging load, it proceeds to dimensions/facts, etc. If any critical failure is encountered in a step, the pipeline could either fail fast or route to a different path (e.g., send email and skip remaining steps).</p></li><li><p><strong>Closing the Batch:</strong> At the very end, the master pipeline will call another stored procedure to mark the batch as completed in the log (and possibly record overall success/failure and timing). This ensures the log has an overview entry indicating the end of the batch. The log tables likely include something like <code>[ETL].[LogOverview]</code> which the notes mention – each batch run gets an overview record with counts of successes, failures, etc., for that batch.</p></li></ol><p>Throughout the master pipeline, logging and error handling are built-in. Even if a sub-pipeline fails, the master might still execute the Send Email step (in a failure path) to alert the team.</p><p><strong>Note on Triggers:</strong> The master pipeline would be the one attached to a trigger (e.g., scheduled nightly). However, during development and testing, you can manually run the master pipeline or even individual sub-pipelines. Always ensure that when testing, you supply required parameters (like Batch ID or others) or temporarily modify the logic if needed. In production, the trigger will supply nothing (just start the master fresh, which itself will create the batch ID internally).</p><h3 id="DataplatformDevOpsWorkshopDay1-part1-FrameworkDatabaseandMetadata-DrivenLoads">Framework Database and Metadata-Driven Loads</h3><p>A key aspect of this system is the <strong>metadata-driven design</strong>. The framework DB contains tables that define:</p><ul><li><p><strong>Data Sources</strong>: A table (e.g., <code>[Metadata].[DataSources]</code>) listing each source system with an ID (like DataSourceID 17 for a specific source, as mentioned in notes). This table might include source name, type, maybe the linked service or connection info (though connection is in LS, but an ID to tie them).</p></li><li><p><strong>Data Objects (Tables)</strong>: A table (e.g., <code>[Metadata].[DataObjects]</code> or perhaps <code>BIML.DataObjects</code>) that lists every source table or data entity that we want to load. It probably has columns for source schema, source table name, maybe target table name, and flags like “is active”, etc.</p></li><li><p><strong>DataLoad (or Load Definitions)</strong>: A table, possibly <code>BIML.DataLoad</code> or <code>BIML.DataLoadDetails</code>, which defines how each table is loaded. For example, it might have DataLoadID as a primary key, a reference to the DataSource and DataObject, and fields such as <em>LoadType</em> (full load vs incremental), <em>Incremental Column</em> (the column used for high-water-mark), <em>Batch Frequency</em>, etc. The mention in notes is that <code>BIML.DataLoadDetails</code><strong> holds the High-Water-Mark logic</strong>, so likely it stores the last loaded value (e.g., last timestamp or last ID loaded for incremental loads) or the query needed to fetch new data.</p></li><li><p><strong>Log Tables</strong>: There are likely tables like <code>[ETL].[LogOverview]</code> and <code>[ETL].[LogDetails]</code> (or similar) which track each pipeline run. For example, LogOverview might have one row per pipeline run (with BatchID, start/end time, status), and LogDetails might have one per table loaded or per step, capturing row counts, durations, errors, etc. The pipeline uses these to record progress and for monitoring.</p></li></ul><p>This metadata approach means the pipelines themselves are generic – they read from these tables to know what to do rather than having hard-coded table names or queries.</p><h3 id="DataplatformDevOpsWorkshopDay1-part1-“LoadSources–ALL”Pipeline(LoadingfromSourcestoODS)">“Load Sources – ALL” Pipeline (Loading from Sources to ODS)</h3><p>The <strong>Load Sources – ALL</strong> pipeline is likely the one that handles extracting data from each source system into the ODS. </p><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041834.png" width="760" loading="lazy" src="attachments/3255730205/3255730231.png?width=760" data-image-src="attachments/3255730205/3255730231.png" data-height="789" data-width="1241" data-unresolved-comment-count="0" data-linked-resource-id="3255730231" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041834.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="ca1592d6-c2f4-4a9c-be15-1e74149895c4" data-media-type="file"></span><p /><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041607.png" width="760" loading="lazy" src="attachments/3255730205/3256025138.png?width=760" data-image-src="attachments/3255730205/3256025138.png" data-height="652" data-width="1267" data-unresolved-comment-count="0" data-linked-resource-id="3256025138" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041607.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="eaa174fc-2541-4246-aa33-c1fc0066dc3c" data-media-type="file"></span><p /><p>All Sources Load are copy pastes Pipelie nso it perform the poarralele laod in a simialr pattern.</p><p>Its structure might be:</p><p>Alwasy start wit ha batch and then wit ha log ID</p><ul><li><p><strong>ForEach over Data Sources:</strong> It could loop through each active DataSource (or it might explicitly call pipelines for each source). However, given the number of sources seems manageable, it might actually just call another pipeline per source. In the related items screenshot, we saw pipelines named <strong>Metadata Discovery for [Source]</strong> and possibly similar patterns for loads. It’s possible that for each source, there is a dedicated pipeline to load its tables. But the phrasing “Load Sources – ALL” suggests it might orchestrate all sources in parallel.</p></li><li><p><strong>Concurrency Control:</strong> A setting on the ForEach or on the pipeline can control concurrency (e.g., run up to N source loads in parallel). The notes mention <em>“Concurrency should be set at the <strong>Load Sources – ALL</strong> level”</em>, implying that within this pipeline they control how many source pipelines run simultaneously to balance load on the IR or database.</p></li><li><p><strong>Execute Source Load pipelines:</strong> For each source, it probably calls a child pipeline (like “Load Source – CRM”, “Load Source – ACE”, etc.) or uses a single pipeline but passes different DataSourceID as a parameter. The simpler design is one generic pipeline that loads one source given its ID, and the master loops through IDs calling the same pipeline with different parameters. Alternatively, separate pipelines per source (which might be overkill and duplicate logic). Given the heavy use of metadata, the more likely scenario is one generic pipeline called with a DataSourceID.</p></li><li><p><strong>Within each Source Load pipeline:</strong> This pipeline will retrieve the list of tables (DataLoadIDs) for that source from the metadata. Possibly a Lookup or Stored Procedure is used at the start to fetch all DataLoadIDs for the given DataSource that need to be processed. Then it will iterate (ForEach) over those tables. That brings us to the next level:</p></li></ul><h3 id="DataplatformDevOpsWorkshopDay1-part1-TableLoadPipeline(LowestLevel:ProcessingOneTable)">Table Load Pipeline (Lowest Level: Processing One Table)</h3><p>The most detailed level of the pipeline framework is the <strong>table-level loading pipeline</strong> – this is executed for each table that needs to be ingested. In the notes it’s referred to as <strong>“Process Table Load”</strong> or appears as part of a pipeline called <strong>“Load tables – ODS – ”</strong>. This is where the actual copying of data happens.</p><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041940.png" width="304" loading="lazy" src="attachments/3255730205/3255926825.png?width=304" data-image-src="attachments/3255730205/3255926825.png" data-height="619" data-width="304" data-unresolved-comment-count="0" data-linked-resource-id="3255926825" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041940.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="a05b8b6e-4bd9-4dcf-b0c6-bbd28ed33cb3" data-media-type="file"></span><p /><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041630.png" width="760" loading="lazy" src="attachments/3255730205/3255992362.png?width=760" data-image-src="attachments/3255730205/3255992362.png" data-height="683" data-width="1599" data-unresolved-comment-count="0" data-linked-resource-id="3255992362" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041630.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="52f3e4b1-5ad9-4637-a68d-c20601996f20" data-media-type="file"></span><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041921.png" width="760" loading="lazy" src="attachments/3255730205/3256090654.png?width=760" data-image-src="attachments/3255730205/3256090654.png" data-height="760" data-width="1077" data-unresolved-comment-count="0" data-linked-resource-id="3256090654" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041921.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="a214ace2-21be-4409-9e4a-60265c6bd69a" data-media-type="file"></span><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041932.png" width="760" loading="lazy" src="attachments/3255730205/3256057889.png?width=760" data-image-src="attachments/3255730205/3256057889.png" data-height="877" data-width="1014" data-unresolved-comment-count="0" data-linked-resource-id="3256057889" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041932.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="81610d2f-e0bd-4992-9839-9b3ea92ea488" data-media-type="file"></span><p /><p /><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041640.png" width="760" loading="lazy" src="attachments/3255730205/3256123431.png?width=760" data-image-src="attachments/3255730205/3256123431.png" data-height="657" data-width="1264" data-unresolved-comment-count="0" data-linked-resource-id="3256123431" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041640.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="46e18a75-8235-41d5-a83c-5383594b3f6c" data-media-type="file"></span><p /><p>Key characteristics and steps of this pipeline:</p><ul><li><p><strong>Parameters:</strong> It typically has at least two pipeline parameters:</p><ol start="1"><li><p><strong>DataLoadID</strong> – an identifier that ties to the metadata for a specific source table (from the <code>BIML.DataLoad</code> table). Using this ID, the pipeline can look up all needed info about that table (source name, table name, whether incremental, etc.).</p></li><li><p><strong>Parent Batch ID</strong> – to link log entries to the overall batch. This is passed down from the master pipeline which created the batch record. Every table load that runs as part of that batch will use this ID so that in the log you can see which batch it belonged to.</p></li></ol></li><li><p><strong>Initialize Logging for Table:</strong> The pipeline likely starts by inserting a log entry for this specific table load (perhaps calling a proc like <code>ETL.StartLogDetail</code> with the BatchID and DataLoadID to create a unique LogID for this load). This helps track when the table load started.</p></li><li><p><strong>Retrieve Metadata for Table:</strong> A Lookup activity or Stored Procedure might be used to fetch metadata from the framework DB for this DataLoadID. Information it would retrieve includes:</p><ul><li><p>Source linked service or source connection (though that’s usually constant per source and already configured in dataset/linked service).</p></li><li><p>Source schema and table name.</p></li><li><p>A custom query if defined (some tables might not be straight table loads but via a <code>SourceQuery</code> that could join or filter source data).</p></li><li><p>Load type (e.g., Full load vs Incremental).</p></li><li><p>If incremental, what column to use as watermark and possibly the last watermark value.</p></li><li><p>Target table name (in ODS) and perhaps staging table name if using a stage.</p></li></ul></li></ul><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041716.png" width="760" loading="lazy" src="attachments/3255730205/3256025151.png?width=760" data-image-src="attachments/3255730205/3256025151.png" data-height="133" data-width="1303" data-unresolved-comment-count="0" data-linked-resource-id="3256025151" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041716.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="e26ebd0b-5d44-41ef-a273-75b60ce90b01" data-media-type="file"></span><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041711.png" width="561" loading="lazy" src="attachments/3255730205/3256025144.png?width=561" data-image-src="attachments/3255730205/3256025144.png" data-height="766" data-width="561" data-unresolved-comment-count="0" data-linked-resource-id="3256025144" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041711.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="935e4c18-ba51-42db-8f4f-f22e1977aea0" data-media-type="file"></span><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041731.png" width="760" loading="lazy" src="attachments/3255730205/3255861306.png?width=760" data-image-src="attachments/3255730205/3255861306.png" data-height="874" data-width="1585" data-unresolved-comment-count="0" data-linked-resource-id="3255861306" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041731.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="9e8a063f-b51b-451f-8123-3271b24e8ae1" data-media-type="file"></span><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041749.png" width="297" loading="lazy" src="attachments/3255730205/3255992369.png?width=297" data-image-src="attachments/3255730205/3255992369.png" data-height="202" data-width="297" data-unresolved-comment-count="0" data-linked-resource-id="3255992369" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041749.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="47ccbd37-513f-493e-9c5a-0c95c15ac585" data-media-type="file"></span><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041754.png" width="249" loading="lazy" src="attachments/3255730205/3256156183.png?width=249" data-image-src="attachments/3255730205/3256156183.png" data-height="286" data-width="249" data-unresolved-comment-count="0" data-linked-resource-id="3256156183" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041754.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="1e0b1934-13a9-400b-8394-3a6e22076121" data-media-type="file"></span><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041759.png" width="205" loading="lazy" src="attachments/3255730205/3255828545.png?width=205" data-image-src="attachments/3255730205/3255828545.png" data-height="118" data-width="205" data-unresolved-comment-count="0" data-linked-resource-id="3255828545" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041759.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="91b66c7e-083e-4360-bacd-824e61227667" data-media-type="file"></span><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041803.png" width="238" loading="lazy" src="attachments/3255730205/3256057883.png?width=238" data-image-src="attachments/3255730205/3256057883.png" data-height="117" data-width="238" data-unresolved-comment-count="0" data-linked-resource-id="3256057883" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041803.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="4e8bf59e-68bd-4e3d-a3f1-0703ca70ad34" data-media-type="file"></span><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-041812.png" width="348" loading="lazy" src="attachments/3255730205/3256123438.png?width=348" data-image-src="attachments/3255730205/3256123438.png" data-height="406" data-width="348" data-unresolved-comment-count="0" data-linked-resource-id="3256123438" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-041812.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="5c8276fe-7c8f-4dee-8569-1b3ae00d9b7b" data-media-type="file"></span><ul><li><p>The note explicitly mentions looking for <code>SourceTableName</code>, <code>SourceSchemaName</code>, and <code>SourceQuery</code>. The presence of <code>SourceQuery</code> allows for flexibility – if a table needs a special SELECT (for instance, to do a complex join or filter out soft-deleted records), it can be stored in the metadata rather than hard-coded in pipeline.</p></li><li><p><strong>High-Water Mark Logic (Incremental Loads):</strong> If the table’s load type is incremental, the pipeline needs to determine the <strong>high-water mark (HWM)</strong> value (e.g., the last loaded timestamp or ID). The process typically is:</p><ol start="1"><li><p><strong>Get Last Loaded Value:</strong> Possibly call a stored procedure in the framework DB that, for a given DataLoadID, returns the max value of the watermark column from the target (or from a control table). The notes mention a stored proc that selects <code>MAX(TimeStamp)</code> from the target. This assumes the target ODS table keeps all history or at least that column’s history.</p><ul><li><p>Alternatively, the framework might store the last watermark in its metadata table (some ETL frameworks maintain a control table of last loaded values for each table).</p></li></ul></li><li><p><strong>Set the Source Query for Incremental:</strong> Once the last HWM value is obtained (say it’s <code>last_timestamp</code>), the pipeline will modify or construct the <code>SourceQuery</code> such that it only pulls records newer than that. For example, <code>SELECT * FROM SourceTable WHERE ModifiedDate &gt; '2023-10-01 00:00:00'</code> (if last run was up to Oct 1, 2023). This dynamic query construction likely happens in the pipeline, possibly using a Set Variable activity: it might take a base query or table name and append a filter on the watermark column using the value fetched.</p><ul><li><p>If no last value exists (e.g., first load), it might default to a full load or use a very old date to get all data.</p></li></ul></li><li><p><strong>Updating Watermark Logic:</strong> After a successful load, the new max timestamp from the source will become the next watermark. The pipeline or a post-load stored proc could update the <code>BIML.DataLoadDetails</code> table with this new value for reference in the next run.</p></li></ol><p /></li><li><p>The workshop discussed complexities here: <em>“50 tables – then we need to set the timestamp field individually (drop and recreate procs each time)”</em> – this implies that originally the approach to get HWM was via table-specific stored procs. </p></li><li><p>A more maintainable approach could be a generic proc that accepts table name &amp; column to avoid having many nearly identical procs. It was also mentioned that altering these procs is heavy and that one should <em>drop and recreate rather than manually altering</em> if needed, keeping them in source control to manage complexity.</p><p>Another nuance: <strong>Dimension tables vs Fact tables.</strong> It was noted that some tables (like dimensions in a data warehouse context) might use two watermark fields or have slowly changing logic, whereas some fact tables are always truncated and reloaded fully (perhaps due to complexity or low volume). </p></li><li><p>The framework seems flexible to accommodate both. For dimension loads, incremental logic must ensure not to miss updates or cause loss of history (for SCD type 2 dims, you wouldn’t usually simply filter by ModifiedDate – you’d need more elaborate change capture). In this framework, if a dimension is treated as incremental, it likely uses <strong><span style="background-color: rgb(211,241,167);">a &quot;RowVersion merge&quot; </span></strong>approach mentioned in notes (maybe utilizing a rowversion or an updated timestamp to merge changes). </p></li><li><p>Fact tables might be loaded full if the volume is manageable, to ensure data consistency (especially if backdated corrections in source are possible – a full reload ensures the fact table is reconciling everything).</p></li><li><p><strong>Data Copy (Source to Staging/ODS):</strong> The main work of the pipeline is done by a <strong>Copy Data activity</strong>. This will use:</p></li><li><p>[Load Tables - ODS - Chelmer ACE &gt;&gt; Load Tables]</p></li><li><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-042319.png" width="736" loading="lazy" src="attachments/3255730205/3255992377.png?width=736" data-image-src="attachments/3255730205/3255992377.png" data-height="806" data-width="1607" data-unresolved-comment-count="0" data-linked-resource-id="3255992377" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-042319.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="1a0a9624-e2a3-48aa-bbea-14829a193c8b" data-media-type="file"></span><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-042333.png" width="400" loading="lazy" src="attachments/3255730205/3256123444.png?width=400" data-image-src="attachments/3255730205/3256123444.png" data-height="302" data-width="400" data-unresolved-comment-count="0" data-linked-resource-id="3256123444" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-042333.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="4519f67c-0b4c-4e18-b211-d28f078d8f7a" data-media-type="file"></span><p /></li><li><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-042248.png" width="736" loading="lazy" src="attachments/3255730205/3256057895.png?width=736" data-image-src="attachments/3255730205/3256057895.png" data-height="773" data-width="1615" data-unresolved-comment-count="0" data-linked-resource-id="3256057895" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-042248.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="eaa8aa09-8958-46e6-ac98-0db979339b3a" data-media-type="file"></span></li><li><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-042237.png" width="736" loading="lazy" src="attachments/3255730205/3255828552.png?width=736" data-image-src="attachments/3255730205/3255828552.png" data-height="362" data-width="931" data-unresolved-comment-count="0" data-linked-resource-id="3255828552" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-042237.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="7600cf17-bfe9-4424-90ad-5590a80723ca" data-media-type="file"></span><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-042413.png" width="736" loading="lazy" src="attachments/3255730205/3256156189.png?width=736" data-image-src="attachments/3255730205/3256156189.png" data-height="727" data-width="1096" data-unresolved-comment-count="0" data-linked-resource-id="3256156189" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-042413.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="89e1b961-6485-41f3-9201-5322c0086519" data-media-type="file"></span></li><li><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-042441.png" width="326" loading="lazy" src="attachments/3255730205/3255992383.png?width=326" data-image-src="attachments/3255730205/3255992383.png" data-height="213" data-width="326" data-unresolved-comment-count="0" data-linked-resource-id="3255992383" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-042441.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="9f4e4742-5f82-43bb-b0fd-927e23d12de2" data-media-type="file"></span><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-042456.png" width="736" loading="lazy" src="attachments/3255730205/3255861320.png?width=736" data-image-src="attachments/3255730205/3255861320.png" data-height="640" data-width="937" data-unresolved-comment-count="0" data-linked-resource-id="3255861320" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-042456.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="6c0158f0-5255-47d1-9ccc-00dc9746d189" data-media-type="file"></span></li><li><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-042507.png" width="421" loading="lazy" src="attachments/3255730205/3255828560.png?width=421" data-image-src="attachments/3255730205/3255828560.png" data-height="586" data-width="421" data-unresolved-comment-count="0" data-linked-resource-id="3255828560" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-042507.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="bd6e39c0-3fa5-4c46-9de4-12568c3f9cd9" data-media-type="file"></span><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-042521.png" width="736" loading="lazy" src="attachments/3255730205/3255992389.png?width=736" data-image-src="attachments/3255730205/3255992389.png" data-height="763" data-width="1254" data-unresolved-comment-count="0" data-linked-resource-id="3255992389" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-042521.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="ef5abf16-c1dd-4f35-b451-145ad9f363dc" data-media-type="file"></span></li></ul><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-042532.png" width="154" loading="lazy" src="attachments/3255730205/3255959582.png?width=154" data-image-src="attachments/3255730205/3255959582.png" data-height="99" data-width="154" data-unresolved-comment-count="0" data-linked-resource-id="3255959582" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-042532.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="83b96bc4-ca3a-4702-b67f-03e4c18fe7f3" data-media-type="file"></span><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-042538.png" width="379" loading="lazy" src="attachments/3255730205/3256090662.png?width=379" data-image-src="attachments/3255730205/3256090662.png" data-height="171" data-width="379" data-unresolved-comment-count="0" data-linked-resource-id="3256090662" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-042538.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="32c47c46-cfe9-46cc-9acf-66c698973b5e" data-media-type="file"></span><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-042543.png" width="556" loading="lazy" src="attachments/3255730205/3255894041.png?width=556" data-image-src="attachments/3255730205/3255894041.png" data-height="351" data-width="556" data-unresolved-comment-count="0" data-linked-resource-id="3255894041" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-042543.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="e6e570b6-2147-4b9d-9936-f5110d6d0078" data-media-type="file"></span><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-042549.png" width="436" loading="lazy" src="attachments/3255730205/3256123456.png?width=436" data-image-src="attachments/3255730205/3256123456.png" data-height="570" data-width="436" data-unresolved-comment-count="0" data-linked-resource-id="3256123456" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-042549.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="28d94590-50d6-487a-af15-8dfc945ba43a" data-media-type="file"></span><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-042556.png" width="549" loading="lazy" src="attachments/3255730205/3255894047.png?width=549" data-image-src="attachments/3255730205/3255894047.png" data-height="286" data-width="549" data-unresolved-comment-count="0" data-linked-resource-id="3255894047" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-042556.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="037debfb-78f5-4f97-8c40-69fdd87cce36" data-media-type="file"></span><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-042604.png" width="373" loading="lazy" src="attachments/3255730205/3256156197.png?width=373" data-image-src="attachments/3255730205/3256156197.png" data-height="94" data-width="373" data-unresolved-comment-count="0" data-linked-resource-id="3256156197" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-042604.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="4f27d599-97bf-440f-a483-e6a4ed12d879" data-media-type="file"></span><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-042613.png" width="760" loading="lazy" src="attachments/3255730205/3255992397.png?width=760" data-image-src="attachments/3255730205/3255992397.png" data-height="736" data-width="1369" data-unresolved-comment-count="0" data-linked-resource-id="3255992397" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-042613.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="872f5a39-4f87-4bcf-97c5-acc39394ac55" data-media-type="file"></span><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-042620.png" width="760" loading="lazy" src="attachments/3255730205/3255959588.png?width=760" data-image-src="attachments/3255730205/3255959588.png" data-height="360" data-width="775" data-unresolved-comment-count="0" data-linked-resource-id="3255959588" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-042620.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="6a07b102-4481-4ed3-9432-bbbb688f6d17" data-media-type="file"></span><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-042626.png" width="664" loading="lazy" src="attachments/3255730205/3256090669.png?width=664" data-image-src="attachments/3255730205/3256090669.png" data-height="576" data-width="664" data-unresolved-comment-count="0" data-linked-resource-id="3256090669" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-042626.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="e984f958-f402-49ec-a365-0a3dacd7f757" data-media-type="file"></span><p /><p /><ul><li><p /><ul><li><p><strong>Source Dataset:</strong> e.g., the parameterized dataset for that source (like <code>ds_CRM</code>), with the parameters SchemaName/TableName (or Query) set appropriately. If using a dynamic SourceQuery, the dataset could be a generic one where you pass the entire query as a parameter. Alternatively, the pipeline might use a <strong>Source</strong> of type “inline” with dynamic SQL. But likely, the design is to use the dataset and just fill in table or query.</p></li><li><p><strong>Sink Dataset:</strong> likely a dataset for the target ODS table. There may be a generic ODS dataset if all ODS tables are in one database (with parameters for table as well). Or they might have one dataset per target area (but since ODS could also have many tables, they might parameterize this too). The sink in DEV would be a DEV ODS (for example, an Azure SQL DB or Synapse Dedicated SQL Pool schema for staging). In higher env, it would point to that env’s ODS.</p></li><li><p>The Copy activity might have <strong>Pre-copy script</strong> or <strong>Post-copy script</strong> if doing truncate or something for full loads. If LoadType is Full (truncate-and-load), the pipeline could call a stored procedure to truncate the target table before copying. If incremental, it might not truncate but rather append/merge (more on merging below).</p></li><li><p><strong>Timeout/Isolation Settings:</strong> They set a high timeout (120 minutes in notes) to allow large data copy. The <em>IsolationLevel</em> property was considered – for certain sources like Oracle or DB2, you might set read consistency isolation. The note mentions DB2 issues with that, suggesting leaving it default if unsure.</p></li><li><p><strong>Partitioning:</strong> If the source is large and supports partitioned copy (e.g., SQL server source can partition by a column to parallelize reading), that can dramatically speed up copy. It requires specifying a partition column and ranges. The framework would need to have that configured per table if used. It wasn’t detailed in notes, so perhaps initially they are not using explicit partitioning except for known large tables.</p></li><li><p><strong>Parallelism in Copy:</strong> The pipelines could run multiple copy activities in parallel if multiple tables are loaded concurrently. However, given each table has its own pipeline instance (through ForEach), the parallelism might be controlled there instead. We need to be mindful not to overload the source or the self-hosted IR.</p></li></ul></li><li><p><strong>Upsert vs Insert Strategy:</strong> After copying data into a staging area, the next step depends on load type:</p><ul><li><p>For <strong>Full load</strong> tables, the simplest method is: truncate the target table, then copy all data fresh. This replaces the content each time. It's straightforward but means downtime during load and requires source providing full data. The pipeline likely handles this by truncating (either via a stored proc activity or a pre-copy script).</p></li><li><p>For <strong>Incremental load</strong> tables, ideally we want to <em>merge</em> new or changed records without duplicating existing ones. Azure Synapse pipelines (ADF) do not have a built-in &quot;UPSERT&quot; in the Copy activity for SQL sinks, except in specific cases (like Cosmos DB or if using Data Flows). Instead, a common approach is:</p><ul><li><p>Copy the new/changed data into a <strong>staging table</strong> (a temporary table or a dedicated staging schema).</p></li><li><p>Then use a Stored Procedure activity to perform a MERGE from staging to target (MERGE SQL statement) or call a set-based UPSERT (like a proc that does UPDATE existing, then INSERT new).</p></li><li><p>Clean up the staging table (delete or truncate it for next run).</p></li></ul></li></ul><p>The notes reference <em>“Misconception of SQL Pool – it is not a SQL database; it doesn’t work the same. Teradata – backend has nothing to do with SQL. Upsert only available for some sources.”</em> </p></li><li><p>This suggests that for the Synapse Dedicated SQL Pool (if that’s the target), some folks might try to use polybase or copy to do an upsert, but that’s not directly supported. </p></li><li><p>Instead, you must load to a staging table and then execute a MERGE or an UPDATE+INSERT. Synapse SQL Pool has specific considerations (e.g., using CTAS or MERGE with distribution keys etc.), but the framework likely abstracts that via stored procedures in the target database.</p><p /></li><li><p>In summary, <strong>for incremental loads, the pipeline likely uses a stored procedure to merge the data</strong>. The mention of <strong>“Look up – Updated Target Table -&gt; then Update Stored Proc”</strong> in the notes indicates:</p></li><li><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-042358.png" width="524" loading="lazy" src="attachments/3255730205/3256123450.png?width=524" data-image-src="attachments/3255730205/3256123450.png" data-height="169" data-width="524" data-unresolved-comment-count="0" data-linked-resource-id="3256123450" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-042358.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="f0fe5a52-b2ff-4905-8902-8a2ac13fa026" data-media-type="file"></span><p /><ul><li><p>They possibly have a Lookup activity to get some info (maybe to check if the target table has data or to compute something like delta count).</p></li><li><p>Then an <strong>Execute Stored Procedure</strong> activity calls a proc to do the final update of the target table using the new data. This proc might be dynamic SQL built based on load type (there was mention of a pattern in the sqldev database to build SQL for incremental merge or full load).</p></li><li><p>The stored proc likely also logs how many rows were inserted/updated, and possibly raises an error if something goes wrong (which the pipeline can catch as a failure).</p></li><li><p /></li></ul></li><li><p><strong>Post-Load Logging:</strong> After the copy (and after the merge if incremental), the pipeline will record the completion in the log:</p><ul><li><p>Update the log detail row for this table with rows read, rows written, duration, status (success/fail), any error message if fail.</p></li><li><p>Possibly update the high-water mark value in the metadata (for incremental loads, store the new max timestamp).</p></li><li><p>Close the log detail entry.</p></li><li><p /></li></ul></li><li><p><strong>Error Handling:</strong> If the Copy activity fails (due to source or network issues, etc.), the pipeline should catch that and mark the log accordingly (with failure status). It might then terminate that branch. Depending on how error handling is set up, the failure could bubble up to fail the source pipeline and ultimately the master pipeline, or they might have some resilience (like skip this table but continue others and flag it). Typically, missing one table might be tolerable for some loads, but usually you’d want any failure to bubble up and stop the process so it can be investigated. The presence of an “Alert_SendMessage_CircuitBreaker” pipeline suggests they do have a mechanism to halt things if failures occur beyond a threshold.</p></li><li><p /></li><li><p><strong>Batch and Pipeline Outcome:</strong> When all tables for a source are done, the source-level pipeline likely ends. Then the Master pipeline will know if any source pipeline failed (maybe via conditional checking of activity outputs) and decide to continue to next steps or not. In any case, once all is done, the Master closes the batch log as mentioned.</p></li></ul><h3 id="DataplatformDevOpsWorkshopDay1-part1-Step5-SummaryofLoggingMechanism">Step 5 - Summary of Logging Mechanism</h3><p>Throughout the above, logging is emphasized. The <strong>IS_Framework</strong> linked service (mentioned as critical) presumably connects to the framework database. If this connection is wrong or down, the whole system can’t log or read metadata, so it’s a linchpin. Ensure that <code>ls_Framework</code> (or similarly named) linked service is configured correctly for each environment (it should point to the environment’s metadata database). In deployment, that connection string is also overridden (via Key Vault or parameters) to the correct DB.</p><p>The logging tables (LogOverview, LogDetail, etc.) allow monitoring of pipeline runs historically. One can build a dashboard or queries on these to see, for instance, average load times per table, row counts, last successful load date per table (which is essentially the high-water mark), and identify any errors quickly. This framework DB can be the source of truth for data operations status.</p><h2 id="DataplatformDevOpsWorkshopDay1-part1-HighlevelOnboardingaNewDataSource–Step-by-Step">High level Onboarding a New Data Source – Step-by-Step</h2><p>When a new source system needs to be integrated into this data platform, there is a defined process to follow so that everything fits into the existing framework.</p><p><strong>Step 1: Set Up Linked Services</strong> – Start by creating any required linked services for the new source. This could include:</p><ul><li><p>A <strong>source linked service</strong> (e.g., <code>ls_Source_NewSystem</code>) pointing to the new source’s database or API. Configure its connection (connection string, authentication). If it’s on-premises, ensure the self-hosted IR can reach it (firewall openings, install any necessary drivers if not SQL). If it’s cloud (say an Azure SQL or another cloud DB), you might use the Azure IR or a different IR if needed.</p></li><li><p>Use Key Vault for credentials: add the new credentials (user/password or connection string) as secrets in each environment’s Key Vault (with consistent naming like <code>SourceNewSystem</code>), and configure the linked service to use those.</p></li><li><p>If the source is of a type not used before (e.g., first time pulling from an FTP or an API), you might need to set up an integration runtime or a compute (for example, a REST linked service for an API, or an SFTP linked service etc.) and test connectivity.</p></li><li><p><strong>Target linked service</strong>: If the data is going into a new target (unlikely; usually all data goes into the same ODS), you might also configure a target LS. In most cases, the existing target (Synapse or SQL DB) is reused so no new one needed.</p></li></ul><p>After creating linked services, <strong>always test the connection</strong> in Synapse Studio to verify network and credentials are correct. This step avoids pipeline runtime failures due to connectivity issues.</p><p><strong>Step 2: Metadata Configuration – Register the Data Source</strong> – In the framework database, you need to add entries for this new source:</p><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-042652.png" width="760" loading="lazy" src="attachments/3255730205/3255861326.png?width=760" data-image-src="attachments/3255730205/3255861326.png" data-height="793" data-width="1063" data-unresolved-comment-count="0" data-linked-resource-id="3255861326" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-042652.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="b6399def-442a-4e93-afbc-868427565400" data-media-type="file"></span><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-042711.png" width="760" loading="lazy" src="attachments/3255730205/3256057903.png?width=760" data-image-src="attachments/3255730205/3256057903.png" data-height="825" data-width="1269" data-unresolved-comment-count="0" data-linked-resource-id="3256057903" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-042711.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="430df750-5cd8-41ef-ae75-46d834a711e9" data-media-type="file"></span><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-042720.png" width="501" loading="lazy" src="attachments/3255730205/3255959594.png?width=501" data-image-src="attachments/3255730205/3255959594.png" data-height="135" data-width="501" data-unresolved-comment-count="0" data-linked-resource-id="3255959594" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-042720.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="2a645ef8-ac37-4511-9cbd-50395c30ce3c" data-media-type="file"></span><p /><p /><ul><li><p>In the <strong>DataSources</strong> table: add a record for the new source (give it a unique DataSourceID, a name, type, maybe the default schema or any needed info). This might also include references to which linked service to use or integration runtime (some frameworks include the IR name or LS name here).</p></li><li><p>Possibly in a <strong>SourceSystem</strong> reference table if one exists.</p></li></ul><p>This entry is important because it will tie together with data objects next. Often the framework will use DataSourceID to know which LS to use (for example, there may be a convention that the linked service name is <code>ls_Source_&lt;System&gt;</code> matching the DataSource name, or there’s a mapping table).</p><p><strong>Step 3: Create Datasets (if needed)</strong> – Check if a dataset already exists that can represent the new source’s data. If the source is the same type as others (e.g., another SQL Server database on-prem), you might reuse a generic dataset like <code>ds_SQLServer_Generic</code> with parameters. However, in our project, it looks like they created separate datasets per source (perhaps to have defaults like the linked service pre-selected). For clarity and isolation, you may:</p><ul><li><p>Clone an existing dataset (say ds_CRM) and rename it to <code>ds_NewSystem</code>. Update its linked service to the new one, and keep the parameters the same (SchemaName, TableName). This dataset can now be used to fetch any table from the new source.</p></li><li><p>If the source uses files (say CSV in ADLS), you’d create a dataset for that (or reuse a generic file dataset if exists).</p></li><li><p>Also consider if a <strong>target dataset</strong> needs updating. If the target is the same (one ODS database), the existing target dataset (which might be generic) can be reused. Usually, there’s something like <code>ds_TargetODS</code> with a parameter for table name.</p></li></ul><p><strong>Step 4: Metadata Discovery (Schema import)</strong> – Before we can load data, the framework needs to know what tables or entities exist in the new source. There is typically a <strong>Metadata Discovery pipeline</strong> (sometimes called &quot;Detect Objects&quot; or &quot;Get Metadata for &quot;). In the provided notes, we see pipelines named <strong>“Metadata Discovery for CRM_ACPR”, “Metadata Discovery for Bloomberg”,</strong> etc., which implies for each source there is a pipeline that reads the source’s schema. The process likely:</p><ul><li><p>Connects to the source (perhaps using the linked service and the built-in GetMetadata activity or queries the source’s information_schema).</p></li><li><p>Retrieves list of tables (and possibly columns) from the source.</p></li><li><p>Writes these into the framework tables like <code>[Metadata].[DataObjects]</code> or a staging from which a procedure populates <code>DataObjects</code>. Fields could include SourceTableName, SourceSchema, perhaps a default load schedule or grouping.</p></li><li><p>Also populates initial entries in <code>BIML.DataLoad</code> or similar, linking those tables with the DataSourceID and default settings (maybe marking them as inactive by default until configured).</p></li></ul><p>Basically, this step automates capturing the source’s schema so you don’t have to manually type in hundreds of table names. After running the metadata discovery for the new source, you should have database records of all tables available from that source.</p><p><strong>Step 5: Configure Data Loads (BIML.DataLoad &amp; DataLoadDetails)</strong> – Now that the framework knows <em>what</em> can be loaded, you must configure <em>how</em> each object should be loaded:</p><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-042824.png" width="760" loading="lazy" src="attachments/3255730205/3255992404.png?width=760" data-image-src="attachments/3255730205/3255992404.png" data-height="748" data-width="1525" data-unresolved-comment-count="0" data-linked-resource-id="3255992404" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-042824.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="e3cd676e-80fc-4457-a909-c8a8917af386" data-media-type="file"></span><p /><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-042736.png" width="760" loading="lazy" src="attachments/3255730205/3256025158.png?width=760" data-image-src="attachments/3255730205/3256025158.png" data-height="510" data-width="1129" data-unresolved-comment-count="0" data-linked-resource-id="3256025158" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-042736.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="d1926cd7-92ed-4a88-9df6-f63ad3f5209b" data-media-type="file"></span><p /><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-042113.png" width="760" loading="lazy" src="attachments/3255730205/3255730237.png?width=760" data-image-src="attachments/3255730205/3255730237.png" data-height="773" data-width="1615" data-unresolved-comment-count="0" data-linked-resource-id="3255730237" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-042113.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="13d44e4c-2e25-46d7-b029-95236c8ec23a" data-media-type="file"></span><span class="confluence-embedded-file-wrapper image-center-wrapper confluence-embedded-manual-size"><img class="confluence-embedded-image image-center" alt="image-20250717-042120.png" width="760" loading="lazy" src="attachments/3255730205/3255861314.png?width=760" data-image-src="attachments/3255730205/3255861314.png" data-height="362" data-width="931" data-unresolved-comment-count="0" data-linked-resource-id="3255861314" data-linked-resource-version="1" data-linked-resource-type="attachment" data-linked-resource-default-alias="image-20250717-042120.png" data-base-url="https://craigsip.atlassian.net/wiki" data-linked-resource-content-type="image/png" data-linked-resource-container-id="3255730205" data-linked-resource-container-version="3" data-media-id="3bc12d2f-a69e-417a-8ab6-db6f9b2af5f9" data-media-type="file"></span><p /><ul><li><p>In the <strong>DataLoad (or DataLoadDetails) table</strong>, find the entries corresponding to the new DataSource’s tables. You’ll likely filter by DataSourceID = (your new source’s ID). Set the appropriate values:</p><ul><li><p><strong>LoadType</strong>: decide for each table if it’s a full load or incremental. For initial onboarding, one might start with full load for simplicity, especially if there isn’t a clear incremental key or if data volume is small. For large tables, identify if there’s a LastModified timestamp or similar for incremental load.</p></li><li><p><strong>Watermark Column</strong>: if incremental, specify the column name (e.g., “ModifiedDate” or “LastUpdated”) to track. Also specify if you need a separate watermark for deletes or something (some advanced scenarios use two marks: one for inserts/updates, one for deletes if soft deletes need handling).</p></li><li><p><strong>Load Frequency / Trigger</strong>: possibly mark whether this table is part of daily load or maybe some tables are monthly etc. The framework might allow grouping tables or indicating if a table should be skipped in certain runs.</p></li><li><p><strong>Source Query</strong> (if needed): If a table needs a custom SQL (maybe to join lookup tables or filter columns), input that query text in the designated field.</p></li><li><p><strong>Target Schema/Table</strong>: ensure the target table name/path is correct. The convention might be to use the same name as source or some prefix like staging schema per source in the ODS.</p></li><li><p><strong>Dependencies</strong>: if any (rare at this raw stage, but e.g., maybe you want one table to load before another if they’re related – though usually ODS loads are independent).</p></li></ul></li><li><p>If the framework separates a DataLoad and DataLoadDetails: DataLoad might be high-level (one per source table), and DataLoadDetails could be per table per environment or for history of loads. Ensure the main config is set in whichever table is the master definition.</p></li></ul><p><strong>Step 6: Create Target Structures (Staging Tables, Stored Procs)</strong> – For each new source table, the corresponding target table (in the ODS or staging database) and possibly a staging table need to exist:</p><ul><li><p>The framework likely has a pipeline called <strong>“Framework – Object Creation”</strong> (as seen in the pipeline list)【16†】. This pipeline might read the metadata (from DataObjects) and auto-generate the necessary SQL objects in the target database:</p><ul><li><p>It could create an empty staging table matching the schema of the source table (perhaps prefixing with the source name or putting in a schema named after the source).</p></li><li><p>Create the final ODS table (if different from staging) or mark that the staging itself is final.</p></li><li><p>Create stored procedures for high-water-mark calculation or for merging if needed.</p></li></ul><p>The notes indicate: <em>“If massive schema changes – only triggering Detect Objects won’t do everything – then run the metadata for that.”</em> This suggests that if you add a lot of tables or change many, you should run the object creation process. It likely compares what’s in metadata vs what’s physically in the database and creates missing ones. It might not handle complex changes (like altering existing table columns) automatically – those would need manual intervention or regeneration of objects.</p></li><li><p>If there isn’t an automated pipeline to create objects, you’d manually execute the provided SQL generation scripts (maybe they have a BimlScript or T-SQL script that uses the metadata to create tables). Given the mention of BIML (which is traditionally used to generate SSIS packages or database scripts), they might have a Biml-based engine originally. But in Azure, they may have translated that to dynamic SQL procedures.</p></li><li><p>In any case, ensure for each DataLoad entry:</p><ul><li><p>The target table exists in the ODS with the correct structure (all columns, data types).</p></li><li><p>If incremental, ensure there is an index on the watermark column (for performance of the MAX and merge).</p></li><li><p>If using a staging table for merge, ensure it exists (could be just a truncated table used each run).</p></li><li><p>The stored procedure for merging (if one per table) is created or a generic merge proc is ready. Some frameworks create a dedicated merge proc per table (naming like <code>usp_Merge_&lt;table&gt;</code>), others use a generic one that uses dynamic SQL.</p></li><li><p>Any reference data needed (like a lookup table for surrogate keys if dimension, etc.) is prepared.</p></li></ul></li></ul><p><strong>Step 7: Testing in DEV</strong> – With all config and objects in place, run the load in DEV:</p><ul><li><p>You can manually trigger the <strong>Metadata Discovery for NewSource</strong> (if not already run) to ensure all metadata present.</p></li><li><p>Run the <strong>Load Sources – ALL</strong> pipeline, but perhaps limit to just your new source (one way is to set others to inactive in metadata or temporarily adjust the pipeline to filter by DataSource). Alternatively, run a specific pipeline for that source if one exists.</p></li><li><p>Monitor the execution: verify that for each table, data is copied into ODS, row counts make sense, and no errors in logs.</p></li><li><p>Check the log tables to ensure each table’s entry is recorded with success and row counts. This is also where you verify high-water-mark behavior: run the pipeline twice and confirm that the second run (incremental) only fetched new changes (e.g., if no new data, it should ideally say 0 rows copied for those tables).</p></li><li><p>If something fails, use the log and pipeline output to debug (common issues could be missing column mapping if source and target schema differ, data type mismatches, Key Vault permission issues, etc.).</p></li></ul><p><strong>Step 8: Deployment to Higher Environments</strong> – Once DEV is working, promote the changes:</p><ul><li><p>Commit all new JSON assets (linked services, datasets, pipeline changes) to Git and merge to main (if in a feature branch).</p></li><li><p>Ensure the Azure DevOps release pipeline has the updated ARM templates including the new linked service, datasets, pipelines, and parameter files updated with any new parameters (for example, new linked service names will appear in the template and likely in the parameters JSON; make sure you add the Prod/UAT connection strings or Key Vault names to those parameter files before running the release).</p></li><li><p>Run the deployment to UAT, then PREPROD, then PROD in sequence (depending on the organization's process).</p></li><li><p>In each environment, after deployment, add the new source’s credentials to the Key Vault of that environment (if not already done) and create any necessary access policies so Synapse (managed identity or service principal) can read the secrets.</p></li><li><p>Execute the pipeline in those environments (perhaps initial loads in a controlled way) and verify everything again. Ideally the first runs are monitored by the team.</p></li></ul><p>Following these steps ensures a smooth integration of a new source with minimal manual tweaking in each environment, thanks to the metadata-driven and parameterized approach.</p><h2 id="DataplatformDevOpsWorkshopDay1-part1-KeyPointsandBestPractices">Key Points and Best Practices</h2><ul><li><p><strong>Metadata-Driven Framework:</strong> The entire data loading process is driven by metadata stored in a central framework database (sometimes called <strong>BIML framework</strong>). This means adding new tables or sources is mostly a matter of updating metadata and not redesigning pipelines. Leverage this by carefully maintaining the metadata (DataSources, DataLoadDetails, etc.) – it is the brain of the operation.</p></li><li><p><code>ls_Framework</code><strong> Linked Service:</strong> This linked service (pointing to the framework/metadata SQL Database) is critical. If this connection fails, pipelines cannot read configuration or log progress. Always verify after deployments that <code>ls_Framework</code> points to the correct database in each environment and that the credentials (likely a service user) are valid. It should ideally use Key Vault like others for its connection string.</p></li><li><p><strong>High Water Mark (HWM) for Incremental Loads:</strong> The incremental loading uses a high-water mark strategy to pull only new or changed records since the last load. This is implemented via stored procedures and metadata in the framework. It’s efficient but requires that the source has a reliable timestamp or incrementing column. Ensure each incremental table’s HWM column is indexed and has no gaps in data capture. Periodically, you may need a full reload to reset baseline or recover from source schema changes – the framework should allow switching the load type to full temporarily. Also, consider edge cases: if the source’s clock changes or if late-arriving data with old timestamps can appear (that would be missed by a naive “greater than last max” filter). In such cases, you might adjust the logic to “greater than last max minus a small delta” or use change tracking if available.</p></li><li><p><strong>Tumbling Window and Complex Scheduling:</strong> For unusual schedules (e.g., business day-specific runs or multiple times a day with dependencies), tumbling window triggers or custom scheduling logic can be used. A tumbling window trigger ensures sequential execution of pipeline windows without overlap. You can combine this with checking a calendar table or API to see if the current day is the Nth working day, etc. The key is to centralize such logic (avoid hardcoding dates in multiple pipelines). If truly complex, sometimes orchestrating via Azure Data Factory’s event-based trigger or even Azure Logic Apps/Functions to invoke pipelines on a schedule might be warranted, but that’s beyond current scope.</p></li><li><p><strong>Consistent Naming and Organization:</strong> The project is organized into folders (as seen: Framework - Execution, Framework - Gather Metadata, etc.). Continue using clear naming conventions for pipelines, datasets, and linked services. For example, all sources have <code>ls_Source_&lt;Name&gt;</code> and corresponding datasets <code>ds_&lt;Name&gt;</code>. This makes it obvious what each resource is for. Triggers should also have meaningful names (instead of auto-generated GUID names) so that it’s clear in the UI/monitor what schedule they represent.</p></li><li><p><strong>DevOps Parameterization:</strong> Use Azure DevOps ARM template parameter overrides judiciously. Do not hardcode environment-specific values in pipelines; instead, use parameters, Key Vault, or global parameters. The deployment pipelines are set up to replace things like Key Vault URLs, storage account names, database names, etc., between environments. If a new parameter is introduced (for a new linked service or trigger, etc.), update the DevOps pipeline definition or parameter JSON files rather than modifying the generated ARM template by hand each time. This upfront work prevents deployment errors where something works in Dev but fails in Test due to a forgotten config change.</p></li><li><p><strong>Monitoring and Maintenance:</strong> With 80+ pipelines and presumably dozens of sources, monitoring is crucial. Consider building a Power BI dashboard or using Azure Monitor to track pipeline runs via the logs. The framework DB’s LogOverview table can be exposed for reporting on load successes/failures and performance. This can highlight trends like a particular source taking longer over time (perhaps indicating source performance issues or increasing data volume). Also, schedule periodic reviews of the metadata configuration: some tables might become obsolete or new tables added in source that need inclusion. An audit or cleanup of unused linked services and datasets was mentioned; it’s good practice to remove or disable things that are no longer needed to reduce clutter and confusion.</p></li></ul><p>By adhering to these practices, the data platform will remain robust, flexible, and easier to manage as it scales.</p><h2 id="DataplatformDevOpsWorkshopDay1-part1-Summary">Summary</h2><p>In summary, the <strong>Data Platform Pipeline Framework</strong> provides an organized, metadata-driven approach to ingesting data from multiple sources into a centralized system using Azure Synapse Analytics. We covered the journey from initial setup (creating linked services with integration runtimes and Key Vault for secure credentials), through defining datasets and triggers, to the architecture of the pipelines themselves – which utilize a logging framework and metadata to dynamically load tables either fully or incrementally. The use of a central framework database (in the style of a BIML or metadata repository) is key to enabling this flexibility.</p><p>The end-to-end process for adding new data sources involves configuration over code: once the pipelines are built, most new additions require minimal pipeline changes – instead you configure the metadata, and the existing pipelines adapt to load the new data. This makes the system scalable and consistent. All environments (DEV, UAT, PREPROD, PROD) follow the same process, with DevOps automation handling the differences through parameterization.</p><p>Overall, the workshop underlined the importance of <strong>good design practices</strong> such as environment isolation, version control, incremental load patterns, and clear operational monitoring. By following the outlined steps and best practices, the data engineering team can continuously enhance the data platform (adding sources, adjusting loads) in a controlled and efficient manner, ensuring reliable data delivery to the business while minimizing maintenance overhead.</p>
                    </div>

                                        <div class="pageSection group">
                        <div class="pageSectionHeader">
                            <h2 id="attachments" class="pageSectionTitle">Attachments:</h2>
                        </div>

                        <div class="greybox" align="left">
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255992339.png">image-20250717-040852.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3256156170.png">image-20250717-040922.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255861286.png">image-20250717-040932.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3256090637.png">image-20250717-040949.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255828519.png">image-20250717-041000.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255959565.png">image-20250717-041016.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255795733.png">image-20250717-041020.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255795740.png">image-20250717-041034.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255861292.png">image-20250717-041042.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255959571.png">image-20250717-041050.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255828525.png">image-20250717-041058.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255730219.png">image-20250717-041139.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3256123423.png">image-20250717-041156.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255828531.png">image-20250717-041209.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3256025124.png">image-20250717-041218.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255861299.png">image-20250717-041223.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255926803.png">image-20250717-041306.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255795746.png">image-20250717-041320.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255926810.png">image-20250717-041331.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3256090644.png">image-20250717-041342.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255992349.png">image-20250717-041501.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255828538.png">image-20250717-041510.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3256025130.png">image-20250717-041523.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255992355.png">image-20250717-041534.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255926817.png">image-20250717-041545.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3256025138.png">image-20250717-041607.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255992362.png">image-20250717-041630.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3256123431.png">image-20250717-041640.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3256025144.png">image-20250717-041711.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3256025151.png">image-20250717-041716.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255861306.png">image-20250717-041731.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3256156177.png">image-20250717-041744.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255992369.png">image-20250717-041749.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3256156183.png">image-20250717-041754.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255828545.png">image-20250717-041759.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3256057883.png">image-20250717-041803.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3256123438.png">image-20250717-041812.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255730231.png">image-20250717-041834.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3256090654.png">image-20250717-041921.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3256057889.png">image-20250717-041932.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255926825.png">image-20250717-041940.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255730237.png">image-20250717-042113.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255861314.png">image-20250717-042120.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255828552.png">image-20250717-042237.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3256057895.png">image-20250717-042248.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255992377.png">image-20250717-042319.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3256123444.png">image-20250717-042333.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3256123450.png">image-20250717-042358.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3256156189.png">image-20250717-042413.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255992383.png">image-20250717-042441.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255861320.png">image-20250717-042456.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255828560.png">image-20250717-042507.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255992389.png">image-20250717-042521.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255959582.png">image-20250717-042532.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3256090662.png">image-20250717-042538.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255894041.png">image-20250717-042543.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3256123456.png">image-20250717-042549.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255894047.png">image-20250717-042556.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3256156197.png">image-20250717-042604.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255992397.png">image-20250717-042613.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255959588.png">image-20250717-042620.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3256090669.png">image-20250717-042626.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255861326.png">image-20250717-042652.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3256057903.png">image-20250717-042711.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255959594.png">image-20250717-042720.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3256025158.png">image-20250717-042736.png</a> (image/png)
                                <br/>
                                                            <img src="images/icons/bullet_blue.gif" height="8" width="8" alt=""/>
                                <a href="attachments/3255730205/3255992404.png">image-20250717-042824.png</a> (image/png)
                                <br/>
                                                    </div>
                    </div>
                    
                                                      
                </div>             </div> 
            <div id="footer" role="contentinfo">
                <section class="footer-body">
                    <p>Document generated by Confluence on 18-08-25 20:53</p>
                    <div id="footer-logo"><a href="http://www.atlassian.com/">Atlassian</a></div>
                </section>
            </div>
        </div>     </body>
</html>
